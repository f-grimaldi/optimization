{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input: (100, 4)\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "inp, y = data['data'], data['target']\n",
    "X = inp[y<2]\n",
    "y = y[y<2]\n",
    "y[y==0] = -1\n",
    "print('Shape of input: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 4) (20, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optim\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, y, threshold=0):\n",
    "    y_pred = model.predict(X)\n",
    "    zeros = y_pred[y == -1]\n",
    "    ones = y_pred[y == 1]\n",
    "    y_pred[y_pred>threshold] = 1\n",
    "    y_pred[y_pred<=threshold] = -1\n",
    "    \n",
    "    return y_pred, 1/(1+np.exp(-zeros)), 1/(1+np.exp(-ones))\n",
    "    \n",
    "def visualize_result(model, X, y):\n",
    "    y_pred, zeros, ones = predict(model, X, y)\n",
    "    print('Accuracy score: {}'.format(np.round(accuracy_score(y_pred, y), 3)))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(zeros, 'o', label='Negative')\n",
    "    plt.plot(ones, 'o', label='Positive')\n",
    "    plt.title('Distribution of outputs by classes')\n",
    "    plt.xlabel('Number of example')\n",
    "    plt.ylabel('Model output')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classic Gradient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model.LogisticRegressor(input_size=X_train.shape[1], bias=True)\n",
    "classic = optim.GradientDescent(clf.W, lr=0.1, weight_decay=0.1)\n",
    "print(clf)\n",
    "start = time.time()\n",
    "clf.fit(X=X_train, y=y_train, optimizer=classic, epoch=3, verbose=0, normalized=True)\n",
    "print('Time spent: {}'.format(time.time()-start))\n",
    "visualize_result(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regressor\n",
      "\tNumber of input: 4\n",
      "\tBias:\t\t True\n",
      "\n",
      "Computing loss:\tactivation:-0.7807571429274698\tloss:0.37710614291468686\n",
      "Computing gradient:\tfirst_term:[-5.5 -4.2 -1.4 -0.2 -1. ]\tsecond_term:0.314156727371556\tgradient:[[1.727862   1.31945825 0.43981942 0.06283135 0.31415673]]\n",
      "New weights are: [[-3.75307074 -2.968735   -1.16504291  0.79278819 -1.86629226]]\n",
      "\n",
      "Computing loss:\tactivation:-31.371091840819748\tloss:2.375877272697807e-14\n",
      "Computing gradient:\tfirst_term:[-5.  -3.2 -1.2 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[5.0e-10 3.2e-10 1.2e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-7.8236308  -5.57389344 -2.14197732  0.62996579 -2.68040427]]\n",
      "\n",
      "Computing loss:\tactivation:-73.7604075442092\tloss:73.7604075442092\n",
      "Computing gradient:\tfirst_term:[6.1 2.8 4.  1.3 1. ]\tsecond_term:1.0\tgradient:[[-6.1 -2.8 -4.  -1.3 -1. ]]\n",
      "New weights are: [[-4.00336298 -3.82032788  0.36311633  1.44412123 -2.05413086]]\n",
      "\n",
      "Computing loss:\tactivation:-29.950691748489707\tloss:9.836575998178404e-14\n",
      "Computing gradient:\tfirst_term:[-4.4 -2.9 -1.4 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[4.4e-10 2.9e-10 1.4e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-7.96941981 -6.43431988 -0.89881084  1.26384592 -2.95550741]]\n",
      "\n",
      "Computing loss:\tactivation:-80.39930001635561\tloss:80.39930001635561\n",
      "Computing gradient:\tfirst_term:[6.9 3.1 4.9 1.5 1. ]\tsecond_term:1.0\tgradient:[[-6.9 -3.1 -4.9 -1.5 -1. ]]\n",
      "New weights are: [[-4.21587036 -4.74794259  1.76675326  2.07983493 -2.41151474]]\n",
      "\n",
      "Computing loss:\tactivation:-40.856726585087195\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.7 -3.8 -1.7 -0.3 -1. ]\tsecond_term:1e-10\tgradient:[[5.7e-10 3.8e-10 1.7e-10 3.0e-11 1.0e-10]]\n",
      "New weights are: [[-8.21019455 -7.41082539  0.57546359  1.86960734 -3.11227337]]\n",
      "\n",
      "Computing loss:\tactivation:-62.06306300157216\tloss:62.06306300157216\n",
      "Computing gradient:\tfirst_term:[5.5 2.5 4.  1.3 1. ]\tsecond_term:1.0\tgradient:[[-5.5 -2.5 -4.  -1.3 -1. ]]\n",
      "New weights are: [[-4.50848366 -5.72822953  3.26761696  2.74455719 -2.43923503]]\n",
      "\n",
      "Computing loss:\tactivation:-22.25689583972315\tloss:22.2568958399389\n",
      "Computing gradient:\tfirst_term:[5.  2.  3.5 1.  1. ]\tsecond_term:0.9999999997842489\tgradient:[[-5.  -2.  -3.5 -1.  -1. ]]\n",
      "New weights are: [[-0.70705406 -4.20765769  5.92861769  3.50484311 -1.6789491 ]]\n",
      "\n",
      "Computing loss:\tactivation:-11.182365368895347\tloss:1.3917377377488145e-05\n",
      "Computing gradient:\tfirst_term:[-5.  -3.5 -1.3 -0.3 -1. ]\tsecond_term:1.3917280531239307e-05\tgradient:[[6.95864027e-05 4.87104819e-05 1.80924647e-05 4.17518416e-06\n",
      "  1.39172805e-05]]\n",
      "New weights are: [[-4.65841965 -6.9736136   4.90126264  3.26776117 -2.46922222]]\n",
      "\n",
      "Computing loss:\tactivation:-29.158414790036364\tloss:29.15841479003658\n",
      "Computing gradient:\tfirst_term:[6.7 3.1 4.4 1.4 1. ]\tsecond_term:0.999999999999783\tgradient:[[-6.7 -3.1 -4.4 -1.4 -1. ]]\n",
      "New weights are: [[-0.83627027 -5.20515643  7.41133088  4.06641925 -1.89875217]]\n",
      "\n",
      "Computing loss:\tactivation:-12.48889153040964\tloss:3.7682742703211107e-06\n",
      "Computing gradient:\tfirst_term:[-5.4 -3.9 -1.7 -0.4 -1. ]\tsecond_term:3.7682671704120096e-06\tgradient:[[2.03486427e-05 1.46962420e-05 6.40605419e-06 1.50730687e-06\n",
      "  3.76826717e-06]]\n",
      "New weights are: [[-4.7164458  -8.00750542  6.18979414  3.77899884 -2.61730319]]\n",
      "\n",
      "Computing loss:\tactivation:-39.09572966910973\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-4.6 -3.1 -1.5 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[4.6e-10 3.1e-10 1.5e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[ -8.65743744 -10.66339109   4.90468817   3.60765138  -3.4740405 ]]\n",
      "\n",
      "Computing loss:\tactivation:-70.09620776422375\tloss:70.09620776422375\n",
      "Computing gradient:\tfirst_term:[7.  3.2 4.7 1.4 1. ]\tsecond_term:1.0\tgradient:[[-7.  -3.2 -4.7 -1.4 -1. ]]\n",
      "New weights are: [[-4.84519934 -8.92065368  7.46433375  4.370099   -2.92943506]]\n",
      "\n",
      "Computing loss:\tactivation:-34.2356719763647\tloss:1.332267629550187e-15\n",
      "Computing gradient:\tfirst_term:[-4.5 -2.3 -1.3 -0.3 -1. ]\tsecond_term:1e-10\tgradient:[[4.5e-10 2.3e-10 1.3e-10 3.0e-11 1.0e-10]]\n",
      "New weights are: [[ -9.07320825 -11.08163601   6.24290896   4.08823174  -3.8689926 ]]\n",
      "\n",
      "Computing loss:\tactivation:-50.209214091216\tloss:50.209214091216\n",
      "Computing gradient:\tfirst_term:[5.5 2.6 4.4 1.2 1. ]\tsecond_term:1.0\tgradient:[[-5.5 -2.6 -4.4 -1.2 -1. ]]\n",
      "New weights are: [[-5.48723452 -9.38644842  9.11168794  4.87062601 -3.21699737]]\n",
      "\n",
      "Computing loss:\tactivation:-52.387542353865996\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.3 -3.7 -1.5 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[5.3e-10 3.7e-10 1.5e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[ -9.43455233 -12.14212313   7.99452252   4.72167062  -3.96177432]]\n",
      "\n",
      "Computing loss:\tactivation:-60.869951127412904\tloss:60.869951127412904\n",
      "Computing gradient:\tfirst_term:[6.6 3.  4.4 1.4 1. ]\tsecond_term:1.0\tgradient:[[-6.6 -3.  -4.4 -1.4 -1. ]]\n",
      "New weights are: [[ -5.62098848 -10.40868501  10.53689842   5.5306084   -3.38396161]]\n",
      "\n",
      "Computing loss:\tactivation:-15.008148476348161\tloss:15.008148779767925\n",
      "Computing gradient:\tfirst_term:[6.6 2.9 4.6 1.3 1. ]\tsecond_term:0.9999996965802813\tgradient:[[-6.599998   -2.89999912 -4.5999986  -1.29999961 -0.9999997 ]]\n",
      "New weights are: [[-1.83113823 -8.74344778 13.1783092   6.27709406 -2.80974188]]\n",
      "\n",
      "Computing loss:\tactivation:-28.275661752998623\tloss:5.249134460426363e-13\n",
      "Computing gradient:\tfirst_term:[-4.6 -3.6 -1.  -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[4.6e-10 3.6e-10 1.0e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[ -5.65598134 -11.73680325  12.34682157   6.11079653  -3.64122951]]\n",
      "\n",
      "Computing loss:\tactivation:-48.058238330357725\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-4.9 -3.  -1.4 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[4.9e-10 3.0e-10 1.4e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[ -9.73874766 -14.2364561   11.18031691   5.94415301  -4.47444713]]\n",
      "\n",
      "Computing loss:\tactivation:-91.77926612071698\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.4 -3.7 -1.5 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[5.4e-10 3.7e-10 1.5e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-13.71363633 -16.95999093  10.07618117   5.79693491  -5.21053762]]\n",
      "\n",
      "Computing loss:\tactivation:-82.638111684774\tloss:82.638111684774\n",
      "Computing gradient:\tfirst_term:[6.  2.9 4.5 1.5 1. ]\tsecond_term:1.0\tgradient:[[-6.  -2.9 -4.5 -1.5 -1. ]]\n",
      "New weights are: [[-10.07319204 -15.20044286  12.80651439   6.70704598  -4.60379691]]\n",
      "\n",
      "Computing loss:\tactivation:-78.76198082498345\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-4.3 -3.  -1.1 -0.1 -1. ]\tsecond_term:1e-10\tgradient:[[4.3e-10 3.0e-10 1.1e-10 1.0e-11 1.0e-10]]\n",
      "New weights are: [[-14.0176482  -17.95238902  11.79746746   6.61531444  -5.52111229]]\n",
      "\n",
      "Computing loss:\tactivation:-119.02997671696755\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.1 -3.4 -1.5 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[5.1e-10 3.4e-10 1.5e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-18.00689587 -20.61188746  10.62415932   6.45887336  -6.30331772]]\n",
      "\n",
      "Computing loss:\tactivation:-118.40616934991664\tloss:118.40616934991664\n",
      "Computing gradient:\tfirst_term:[5.7 3.  4.2 1.2 1. ]\tsecond_term:1.0\tgradient:[[-5.7 -3.  -4.2 -1.2 -1. ]]\n",
      "New weights are: [[-14.37477507 -18.70024494  13.30045885   7.22353037  -5.66610354]]\n",
      "\n",
      "Computing loss:\tactivation:-116.9506653956129\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.  -3.4 -1.6 -0.4 -1. ]\tsecond_term:1e-10\tgradient:[[5.0e-10 3.4e-10 1.6e-10 4.0e-11 1.0e-10]]\n",
      "New weights are: [[-18.3138594  -21.37882228  12.03995187   6.90840362  -6.45392041]]\n",
      "\n",
      "Computing loss:\tactivation:-138.41351489221864\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-4.4 -3.2 -1.3 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[4.4e-10 3.2e-10 1.3e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-22.18304736 -24.19277716  10.8967827    6.73253144  -7.33328131]]\n",
      "\n",
      "Computing loss:\tactivation:-198.50243809415807\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.5 -3.5 -1.3 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[5.5e-10 3.5e-10 1.3e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-26.27206892 -26.79488179   9.93028669   6.58383975  -8.07673977]]\n",
      "\n",
      "Computing loss:\tactivation:-238.99669948348722\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.2 -4.1 -1.5 -0.1 -1. ]\tsecond_term:1e-10\tgradient:[[5.2e-10 4.1e-10 1.5e-10 1.0e-11 1.0e-10]]\n",
      "New weights are: [[-30.06012845 -29.78162104   8.83757721   6.51099245  -8.80521276]]\n",
      "\n",
      "Computing loss:\tactivation:-228.62052078495645\tloss:228.62052078495645\n",
      "Computing gradient:\tfirst_term:[5.9 3.  4.2 1.5 1. ]\tsecond_term:1.0\tgradient:[[-5.9 -3.  -4.2 -1.5 -1. ]]\n",
      "New weights are: [[-26.39263054 -27.91679159  11.44833844   7.44340717  -8.18360294]]\n",
      "\n",
      "Computing loss:\tactivation:-203.74753773559686\tloss:203.74753773559686\n",
      "Computing gradient:\tfirst_term:[6.4 3.2 4.5 1.5 1. ]\tsecond_term:1.0\tgradient:[[-6.4 -3.2 -4.5 -1.5 -1. ]]\n",
      "New weights are: [[-22.69017649 -26.06556457  14.05162645   8.31116984  -7.6050945 ]]\n",
      "\n",
      "Computing loss:\tactivation:-175.10783435309952\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.  -3.  -1.6 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[5.0e-10 3.0e-10 1.6e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-26.76722694 -28.51179484  12.7469703    8.14808782  -8.42050459]]\n",
      "\n",
      "Computing loss:\tactivation:-224.43445917816828\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.1 -3.5 -1.4 -0.3 -1. ]\tsecond_term:1e-10\tgradient:[[5.1e-10 3.5e-10 1.4e-10 3.0e-11 1.0e-10]]\n",
      "New weights are: [[-30.73468718 -31.23456167  11.65786357   7.91470781  -9.19843797]]\n",
      "\n",
      "Computing loss:\tactivation:-209.09342137934587\tloss:209.09342137934587\n",
      "Computing gradient:\tfirst_term:[5.7 2.8 4.5 1.3 1. ]\tsecond_term:1.0\tgradient:[[-5.7 -2.8 -4.5 -1.3 -1. ]]\n",
      "New weights are: [[-27.15169432 -29.474495    14.48654214   8.73188162  -8.56984273]]\n",
      "\n",
      "Computing loss:\tactivation:-188.74280571808197\tloss:188.74280571808197\n",
      "Computing gradient:\tfirst_term:[6.2 2.9 4.3 1.3 1. ]\tsecond_term:1.0\tgradient:[[-6.2 -2.9 -4.3 -1.3 -1. ]]\n",
      "New weights are: [[-23.39322112 -27.71649947  17.09322517   9.51994858  -7.96363738]]\n",
      "\n",
      "Computing loss:\tactivation:-156.0012860471022\tloss:156.0012860471022\n",
      "Computing gradient:\tfirst_term:[6.7 3.1 4.7 1.5 1. ]\tsecond_term:1.0\tgradient:[[-6.7 -3.1 -4.7 -1.5 -1. ]]\n",
      "New weights are: [[-19.64405621 -25.98181123  19.72323637  10.35931386  -7.40406053]]\n",
      "\n",
      "Computing loss:\tactivation:-102.13867840971291\tloss:102.13867840971291\n",
      "Computing gradient:\tfirst_term:[5.8 2.7 3.9 1.2 1. ]\tsecond_term:1.0\tgradient:[[-5.8 -2.7 -3.9 -1.2 -1. ]]\n",
      "New weights are: [[-15.85506748 -24.21797165  22.27100466  11.14324256  -6.75078661]]\n",
      "\n",
      "Computing loss:\tactivation:-51.2586946072244\tloss:51.2586946072244\n",
      "Computing gradient:\tfirst_term:[5.6 3.  4.5 1.5 1. ]\tsecond_term:1.0\tgradient:[[-5.6 -3.  -4.5 -1.5 -1. ]]\n",
      "New weights are: [[-12.35123306 -22.34091749  25.08658589  12.08176964  -6.12510189]]\n",
      "\n",
      "Computing loss:\tactivation:-109.77202755820024\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.1 -3.5 -1.4 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[5.1e-10 3.5e-10 1.4e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-16.32109651 -25.06533359  23.99681945  11.92608872  -6.90350649]]\n",
      "\n",
      "Computing loss:\tactivation:-156.8259278525419\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.4 -3.9 -1.3 -0.4 -1. ]\tsecond_term:1e-10\tgradient:[[5.4e-10 3.9e-10 1.3e-10 4.0e-11 1.0e-10]]\n",
      "New weights are: [[-20.25026609 -27.90306717  23.05090826  11.63503912  -7.63113048]]\n",
      "\n",
      "Computing loss:\tactivation:-157.98354559456425\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-5.1 -3.3 -1.7 -0.5 -1. ]\tsecond_term:1e-10\tgradient:[[5.1e-10 3.3e-10 1.7e-10 5.0e-11 1.0e-10]]\n",
      "New weights are: [[-24.23075579 -30.47867815  21.72407836  11.24479503  -8.41161866]]\n",
      "\n",
      "Computing loss:\tactivation:-122.7081368546114\tloss:122.7081368546114\n",
      "Computing gradient:\tfirst_term:[6.  2.2 4.  1.  1. ]\tsecond_term:1.0\tgradient:[[-6.  -2.2 -4.  -1.  -1. ]]\n",
      "New weights are: [[-20.31978184 -29.04465437  24.33139432  11.89662402  -7.75978967]]\n",
      "\n",
      "Computing loss:\tactivation:-84.8436659059574\tloss:84.8436659059574\n",
      "Computing gradient:\tfirst_term:[4.9 2.4 3.3 1.  1. ]\tsecond_term:1.0\tgradient:[[-4.9 -2.4 -3.3 -1.  -1. ]]\n",
      "New weights are: [[-16.56870761 -27.20739352  26.85762799  12.66214938  -6.99426431]]\n",
      "\n",
      "Computing loss:\tactivation:-22.633130744755874\tloss:22.633130744903976\n",
      "Computing gradient:\tfirst_term:[6.  2.7 5.1 1.6 1. ]\tsecond_term:0.9999999998518996\tgradient:[[-6.  -2.7 -5.1 -1.6 -1. ]]\n",
      "New weights are: [[-13.0541014  -25.62582073  29.84504326  13.5993777   -6.40849661]]\n",
      "\n",
      "Computing loss:\tactivation:-102.32619733893128\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-4.9 -3.1 -1.5 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[4.9e-10 3.1e-10 1.5e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-17.08678419 -28.17710984  28.61054853  13.4347784   -7.2314931 ]]\n",
      "\n",
      "Computing loss:\tactivation:-137.82546154830393\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[-4.7 -3.2 -1.3 -0.2 -1. ]\tsecond_term:1e-10\tgradient:[[4.7e-10 3.2e-10 1.3e-10 2.0e-11 1.0e-10]]\n",
      "New weights are: [[-21.0556088  -30.8792883   27.51278853  13.26589225  -8.07592387]]\n",
      "\n",
      "Computing loss:\tactivation:-95.52690241934712\tloss:95.52690241934712\n",
      "Computing gradient:\tfirst_term:[5.1 2.5 3.  1.1 1. ]\tsecond_term:1.0\tgradient:[[-5.1 -2.5 -3.  -1.1 -1. ]]\n",
      "New weights are: [[-17.18797488 -28.98338932  29.78786731  14.1000878   -7.31756428]]\n",
      "\n",
      "Computing loss:\tactivation:-62.05561617850378\tloss:62.05561617850378\n",
      "Computing gradient:\tfirst_term:[5.6 2.9 3.6 1.3 1. ]\tsecond_term:1.0\tgradient:[[-5.6 -2.9 -3.6 -1.3 -1. ]]\n",
      "New weights are: [[-13.42678923 -27.03563246  32.20577237  14.97322018  -6.64592398]]\n",
      "\n",
      "Computing loss:\tactivation:-10.500622260831761\tloss:10.500649779772937\n",
      "Computing gradient:\tfirst_term:[5.8 2.7 4.1 1.  1. ]\tsecond_term:0.9999724814374662\tgradient:[[-5.79984039 -2.6999257  -4.09988717 -0.99997248 -0.99997248]]\n",
      "New weights are: [[ -9.67476712 -25.28900148  34.85806386  15.62012055  -5.99902362]]\n",
      "\n",
      "Computing loss:\tactivation:51.429969142100504\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.9 3.2 4.8 1.8 1. ]\tsecond_term:1e-10\tgradient:[[-5.9e-10 -3.2e-10 -4.8e-10 -1.8e-10 -1.0e-10]]\n",
      "New weights are: [[ -6.20609872 -23.40768981  37.68003137  16.67835836  -5.41111372]]\n",
      "\n",
      "Computing loss:\tactivation:82.02660876490071\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.3 3.3 4.7 1.6 1. ]\tsecond_term:1e-10\tgradient:[[-6.3e-10 -3.3e-10 -4.7e-10 -1.6e-10 -1.0e-10]]\n",
      "New weights are: [[ -2.59825603 -21.51786744  40.37159655  17.59463587  -4.83844028]]\n",
      "\n",
      "Computing loss:\tactivation:-17.990920448984923\tloss:1.5368890708548872e-08\n",
      "Computing gradient:\tfirst_term:[-5.  -3.5 -1.6 -0.6 -1. ]\tsecond_term:1.5368890604605667e-08\tgradient:[[7.68444530e-08 5.37911171e-08 2.45902250e-08 9.22133436e-09\n",
      "  1.53688906e-08]]\n",
      "New weights are: [[ -6.49453079 -24.24525978  39.12478863  17.1270829   -5.61769523]]\n",
      "\n",
      "Computing loss:\tactivation:48.38964698299365\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.7 2.6 3.5 1.  1. ]\tsecond_term:1e-10\tgradient:[[-5.7e-10 -2.6e-10 -3.5e-10 -1.0e-10 -1.0e-10]]\n",
      "New weights are: [[ -2.598091   -22.46793637  41.51733938  17.81066883  -4.9341093 ]]\n",
      "\n",
      "Computing loss:\tactivation:-29.80883578025071\tloss:1.1324274851175956e-13\n",
      "Computing gradient:\tfirst_term:[-4.6 -3.4 -1.4 -0.3 -1. ]\tsecond_term:1e-10\tgradient:[[4.6e-10 3.4e-10 1.4e-10 3.0e-11 1.0e-10]]\n",
      "New weights are: [[ -6.4437287  -25.31036423  40.3469279   17.55986637  -5.77011749]]\n",
      "\n",
      "Computing loss:\tactivation:97.79257439597424\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.8 2.8 4.8 1.4 1. ]\tsecond_term:1e-10\tgradient:[[-6.8e-10 -2.8e-10 -4.8e-10 -1.4e-10 -1.0e-10]]\n",
      "New weights are: [[ -2.64431237 -23.74589868  43.02886884  18.34209914  -5.2113798 ]]\n",
      "\n",
      "Computing loss:\tactivation:121.5895394683951\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.5 2.3 4.  1.3 1. ]\tsecond_term:1e-10\tgradient:[[-5.5e-10 -2.3e-10 -4.0e-10 -1.3e-10 -1.0e-10]]\n",
      "New weights are: [[  1.09001933 -22.18426906  45.74474644  19.22475936  -4.5324104 ]]\n",
      "\n",
      "Computing loss:\tactivation:188.10881559171497\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.2 2.2 4.5 1.5 1. ]\tsecond_term:1e-10\tgradient:[[-6.2e-10 -2.2e-10 -4.5e-10 -1.5e-10 -1.0e-10]]\n",
      "New weights are: [[  4.88350534 -20.83819338  48.49808306  20.14253824  -3.92055782]]\n",
      "\n",
      "Computing loss:\tactivation:197.04119854706772\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.6 2.7 4.2 1.3 1. ]\tsecond_term:1e-10\tgradient:[[-5.6e-10 -2.7e-10 -4.2e-10 -1.3e-10 -1.0e-10]]\n",
      "New weights are: [[  8.52941285 -19.08034511  51.23251369  20.98890962  -3.2695029 ]]\n",
      "\n",
      "Computing loss:\tactivation:58.968309236049976\tloss:58.968309236049976\n",
      "Computing gradient:\tfirst_term:[-4.8 -3.4 -1.6 -0.2 -1. ]\tsecond_term:1.0\tgradient:[[4.8 3.4 1.6 0.2 1. ]]\n",
      "New weights are: [[  4.64630404 -21.83088052  49.93814409  20.82711342  -4.07848391]]\n",
      "\n",
      "Computing loss:\tactivation:34.61449911174225\tloss:34.61449911174225\n",
      "Computing gradient:\tfirst_term:[-4.8 -3.1 -1.6 -0.2 -1. ]\tsecond_term:0.9999999999999991\tgradient:[[4.8 3.1 1.6 0.2 1. ]]\n",
      "New weights are: [[  0.66012101 -24.4052904   48.60941641  20.66102246  -4.9089387 ]]\n",
      "\n",
      "Computing loss:\tactivation:175.17375111089507\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.4 3.  4.5 1.5 1. ]\tsecond_term:1e-10\tgradient:[[-5.4e-10 -3.0e-10 -4.5e-10 -1.5e-10 -1.0e-10]]\n",
      "New weights are: [[  4.09856539 -22.49504352  51.47478673  21.6161459   -4.27218975]]\n",
      "\n",
      "Computing loss:\tactivation:37.59788154098973\tloss:37.59788154098973\n",
      "Computing gradient:\tfirst_term:[-5.1 -3.8 -1.9 -0.4 -1. ]\tsecond_term:1.0\tgradient:[[5.1 3.8 1.9 0.4 1. ]]\n",
      "New weights are: [[  0.306508   -25.32049804  50.06205947  21.31872964  -5.01573041]]\n",
      "\n",
      "Computing loss:\tactivation:153.70801262582427\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.6 3.  4.1 1.3 1. ]\tsecond_term:1e-10\tgradient:[[-5.6e-10 -3.0e-10 -4.1e-10 -1.3e-10 -1.0e-10]]\n",
      "New weights are: [[  3.9255171  -23.38174317  52.71169113  22.15885675  -4.36947878]]\n",
      "\n",
      "Computing loss:\tactivation:222.9271246853456\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.1 3.  4.6 1.4 1. ]\tsecond_term:1e-10\tgradient:[[-6.1e-10 -3.0e-10 -4.6e-10 -1.4e-10 -1.0e-10]]\n",
      "New weights are: [[  7.56240178 -21.59311136  55.45425991  22.99355159  -3.77326818]]\n",
      "\n",
      "Computing loss:\tactivation:214.17078762099598\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.5 2.4 3.7 1.  1. ]\tsecond_term:1e-10\tgradient:[[-5.5e-10 -2.4e-10 -3.7e-10 -1.0e-10 -1.0e-10]]\n",
      "New weights are: [[ 11.38701413 -19.92418961  58.02718094  23.68893566  -3.07788412]]\n",
      "\n",
      "Computing loss:\tactivation:343.84963701164895\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.7 3.  5.  1.7 1. ]\tsecond_term:1e-10\tgradient:[[-6.7e-10 -3.0e-10 -5.0e-10 -1.7e-10 -1.0e-10]]\n",
      "New weights are: [[ 15.06899981 -18.2755393   60.77493145  24.62317083  -2.52833402]]\n",
      "\n",
      "Computing loss:\tactivation:349.7924448883333\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.3 2.3 4.4 1.3 1. ]\tsecond_term:1e-10\tgradient:[[-6.3e-10 -2.3e-10 -4.4e-10 -1.3e-10 -1.0e-10]]\n",
      "New weights are: [[ 18.91647603 -16.87090513  63.4620577   25.41709449  -1.91762351]]\n",
      "\n",
      "Computing loss:\tactivation:137.06967600052337\tloss:137.06967600052337\n",
      "Computing gradient:\tfirst_term:[-5.1 -3.8 -1.6 -0.2 -1. ]\tsecond_term:1.0\tgradient:[[5.1 3.8 1.6 0.2 1. ]]\n",
      "New weights are: [[ 15.07438877 -19.73363681  62.25669699  25.26642441  -2.67097395]]\n",
      "\n",
      "Computing loss:\tactivation:104.6855678513246\tloss:104.6855678513246\n",
      "Computing gradient:\tfirst_term:[-5.1 -3.7 -1.5 -0.4 -1. ]\tsecond_term:1.0\tgradient:[[5.1 3.7 1.5 0.4 1. ]]\n",
      "New weights are: [[ 11.19063964 -22.55125873  61.11441784  24.96181663  -3.43249339]]\n",
      "\n",
      "Computing loss:\tactivation:75.6605472272503\tloss:75.6605472272503\n",
      "Computing gradient:\tfirst_term:[-4.9 -3.1 -1.5 -0.1 -1. ]\tsecond_term:1.0\tgradient:[[4.9 3.1 1.5 0.1 1. ]]\n",
      "New weights are: [[  7.156317   -25.1035853   59.87942111  24.87948352  -4.25582454]]\n",
      "\n",
      "Computing loss:\tactivation:233.53836276998976\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.2 2.7 3.9 1.4 1. ]\tsecond_term:1e-10\tgradient:[[-5.2e-10 -2.7e-10 -3.9e-10 -1.4e-10 -1.0e-10]]\n",
      "New weights are: [[ 10.7446579  -23.24040829  62.57067679  25.8455753   -3.56575898]]\n",
      "\n",
      "Computing loss:\tactivation:300.45602564854374\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.4 2.9 4.3 1.3 1. ]\tsecond_term:1e-10\tgradient:[[-6.4e-10 -2.9e-10 -4.3e-10 -1.3e-10 -1.0e-10]]\n",
      "New weights are: [[ 14.55445169 -21.51409548  65.13038199  26.61943966  -2.9704787 ]]\n",
      "\n",
      "Computing loss:\tactivation:302.2220116425196\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[5.5 2.4 3.8 1.1 1. ]\tsecond_term:1e-10\tgradient:[[-5.5e-10 -2.4e-10 -3.8e-10 -1.1e-10 -1.0e-10]]\n",
      "New weights are: [[ 18.34404205 -19.86045605  67.74864442  27.37735773  -2.28146227]]\n",
      "\n",
      "Computing loss:\tactivation:124.22281672704315\tloss:124.22281672704315\n",
      "Computing gradient:\tfirst_term:[-5.  -3.3 -1.4 -0.2 -1. ]\tsecond_term:1.0\tgradient:[[5.  3.3 1.4 0.2 1. ]]\n",
      "New weights are: [[ 14.33518069 -22.50630455  66.62616324  27.21700328  -3.08323454]]\n",
      "\n",
      "Computing loss:\tactivation:374.38364299376605\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.5 2.8 4.6 1.5 1. ]\tsecond_term:1e-10\tgradient:[[-6.5e-10 -2.8e-10 -4.6e-10 -1.5e-10 -1.0e-10]]\n",
      "New weights are: [[ 18.10052959 -20.8843081   69.29087169  28.08592995  -2.5039501 ]]\n",
      "\n",
      "Computing loss:\tactivation:115.43959504812305\tloss:115.43959504812305\n",
      "Computing gradient:\tfirst_term:[-5.  -3.6 -1.4 -0.2 -1. ]\tsecond_term:1.0\tgradient:[[5.  3.6 1.4 0.2 1. ]]\n",
      "New weights are: [[ 14.19427959 -23.6968081   68.19712169  27.92967995  -3.2852001 ]]\n",
      "\n",
      "Computing loss:\tactivation:102.29945874861741\tloss:102.29945874861741\n",
      "Computing gradient:\tfirst_term:[-4.7 -3.2 -1.6 -0.2 -1. ]\tsecond_term:1.0\tgradient:[[4.7 3.2 1.6 0.2 1. ]]\n",
      "New weights are: [[ 10.27379949 -26.36607115  66.86249017  27.76285101  -4.1193448 ]]\n",
      "\n",
      "Computing loss:\tactivation:57.928761625160355\tloss:57.928761625160355\n",
      "Computing gradient:\tfirst_term:[-4.6 -3.2 -1.4 -0.2 -1. ]\tsecond_term:1.0\tgradient:[[4.6 3.2 1.4 0.2 1. ]]\n",
      "New weights are: [[  6.35233204 -29.0940485   65.66900007  27.59235242  -4.97183772]]\n",
      "\n",
      "Computing loss:\tactivation:38.45151637842778\tloss:38.45151637842778\n",
      "Computing gradient:\tfirst_term:[-4.8 -3.  -1.4 -0.3 -1. ]\tsecond_term:1.0\tgradient:[[4.8 3.  1.4 0.3 1. ]]\n",
      "New weights are: [[  2.30079739 -31.62625766  64.48730247  27.33913151  -5.81590744]]\n",
      "\n",
      "Computing loss:\tactivation:234.395072340464\tloss:0.0\n",
      "Computing gradient:\tfirst_term:[6.  3.4 4.5 1.6 1. ]\tsecond_term:1e-10\tgradient:[[-6.0e-10 -3.4e-10 -4.5e-10 -1.6e-10 -1.0e-10]]\n",
      "New weights are: [[  5.85190151 -29.61396532  67.15063056  28.28609261  -5.22405676]]\n",
      "\n",
      "Step:  0\t\tLoss: nan\n",
      "Time spent: 0.15691280364990234\n",
      "Accuracy score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fgrim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\fgrim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = model.LogisticRegressor(input_size=X_train.shape[1], bias=True)\n",
    "sgd = optim.SGD(clf.W, lr=5, weight_decay=0, verbose=True)\n",
    "print(clf)\n",
    "start = time.time()\n",
    "clf.fit(X=X_train, y=y_train, optimizer=sgd, epoch=1, verbose=1)\n",
    "print('Time spent: {}'.format(time.time()-start))\n",
    "visualize_result(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
